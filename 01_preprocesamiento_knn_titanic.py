# %% [markdown]
"""
# Preprocesamiento del Dataset Titanic con KNN
Este notebook implementa un sistema robusto de imputaci√≥n de valores faltantes usando K-Nearest Neighbors (KNN).

## Objetivos:
1. Cargar y explorar el dataset Titanic
2. Identificar columnas con valores faltantes
3. Aplicar imputaci√≥n KNN por columna de manera sistem√°tica
4. Exportar dataset limpio para an√°lisis posterior

## Limitaciones del enfoque KNN:
- Sensible a la escala de las variables (requiere normalizaci√≥n)
- Computacionalmente costoso para datasets grandes
- Puede introducir patrones que no existen naturalmente en los datos
- La calidad depende del valor de k seleccionado
"""

# %%
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# %% [markdown]
"""
## 1. Carga y Exploraci√≥n Inicial
Cargamos el dataset Titanic y realizamos una exploraci√≥n inicial de nulos y tipos de datos.
"""

# %%
# Cargar dataset Titanic
df = sns.load_dataset("titanic")
print(f"Dimensiones del dataset: {df.shape}")
print("\n" + "="*50)
print("INFORMACI√ìN GENERAL DEL DATASET")
print("="*50)
df.info()

# %%
print("="*50)
print("VALORES NULOS POR COLUMNA (ANTES)")
print("="*50)
missing_before = df.isnull().sum()
missing_pct_before = (df.isnull().sum() / len(df) * 100).round(2)
missing_summary_before = pd.DataFrame({
    'Valores_Nulos': missing_before,
    'Porcentaje': missing_pct_before
}).sort_values('Porcentaje', ascending=False)
print(missing_summary_before[missing_summary_before['Valores_Nulos'] > 0])

# %% [markdown]
"""
## 2. Clasificaci√≥n de Variables
Separamos las columnas en num√©ricas y categ√≥ricas para aplicar diferentes estrategias de imputaci√≥n.
"""

# %%
# Separar columnas por tipo
numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()

print("COLUMNAS NUM√âRICAS:", numeric_columns)
print("COLUMNAS CATEG√ìRICAS:", categorical_columns)

# %% [markdown]
"""
## 3. Estrategia de Imputaci√≥n KNN
Aplicamos KNN columna por columna para evitar circularidad. Para cada columna:
1. Usamos relleno temporal para construir las features
2. Excluimos la columna objetivo del conjunto de caracter√≠sticas  
3. Aplicamos transformaciones apropiadas (StandardScaler + OneHotEncoder)
4. Entrenamos el modelo KNN y predecimos los valores faltantes
"""

# %%
def impute_with_knn(df, target_column, k=5):
    """
    Imputa una columna espec√≠fica usando KNN
    """
    if df[target_column].isnull().sum() == 0:
        return df[target_column].copy()
    
    # Verificar si hay m√°s del 70% de nulos
    null_percentage = df[target_column].isnull().sum() / len(df) * 100
    if null_percentage > 70:
        print(f"Columna '{target_column}' tiene {null_percentage:.1f}% nulos (>70%). Aplicando relleno simple.")
        if target_column in numeric_columns:
            return df[target_column].fillna(df[target_column].median())
        else:
            # Manejar columnas categ√≥ricas
            if pd.api.types.is_categorical_dtype(df[target_column]):
                # Agregar 'missing' a las categor√≠as si no existe
                if 'missing' not in df[target_column].cat.categories:
                    df_result = df[target_column].cat.add_categories(['missing'])
                    return df_result.fillna('missing')
                else:
                    return df[target_column].fillna('missing')
            else:
                return df[target_column].fillna('missing')
    
    print(f"Imputando columna '{target_column}' ({null_percentage:.1f}% nulos) con KNN...")
    
    # Crear copia del DataFrame
    df_temp = df.copy()
    
    # Relleno temporal para construir features
    for col in numeric_columns:
        if col != target_column:
            df_temp[col] = df_temp[col].fillna(df_temp[col].median())
    
    for col in categorical_columns:
        if col != target_column:
            df_temp[col] = df_temp[col].fillna(df_temp[col].mode()[0] if not df_temp[col].mode().empty else 'unknown')
    
    # Preparar caracter√≠sticas (excluir columna objetivo)
    feature_cols = [col for col in df.columns if col != target_column]
    X = df_temp[feature_cols]
    y = df_temp[target_column]
    
    # Separar features por tipo para el transformador
    numeric_features = [col for col in feature_cols if col in numeric_columns]
    categorical_features = [col for col in feature_cols if col in categorical_columns]
    
    # Crear pipeline de preprocessing
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
        ],
        remainder='drop'
    )
    
    # Identificar filas con y sin valores faltantes
    mask_missing = y.isnull()
    mask_complete = ~mask_missing
    
    if mask_complete.sum() == 0:
        print(f"No hay valores no nulos para entrenar en '{target_column}'")
        if target_column in numeric_columns:
            return df[target_column].fillna(0)
        else:
            return df[target_column].fillna('unknown')
    
    # Preparar datos de entrenamiento
    X_train = X[mask_complete]
    y_train = y[mask_complete]
    X_missing = X[mask_missing]
    
    # Procesar caracter√≠sticas
    X_train_processed = preprocessor.fit_transform(X_train)
    X_missing_processed = preprocessor.transform(X_missing)
    
    # Determinar el modelo seg√∫n el tipo de columna
    if target_column in numeric_columns:
        # Para variables num√©ricas usar KNeighborsRegressor
        model = KNeighborsRegressor(n_neighbors=min(k, len(X_train_processed)))
        model.fit(X_train_processed, y_train)
        predictions = model.predict(X_missing_processed)
    else:
        # Para variables categ√≥ricas usar KNeighborsClassifier
        # Codificar la variable objetivo
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train.astype(str))
        
        model = KNeighborsClassifier(n_neighbors=min(k, len(X_train_processed)))
        model.fit(X_train_processed, y_train_encoded)
        predictions_encoded = model.predict(X_missing_processed)
        
        # Decodificar las predicciones
        predictions = label_encoder.inverse_transform(predictions_encoded)
    
    # Crear serie resultado
    result = y.copy()
    result.loc[mask_missing] = predictions
    
    return result

# %% [markdown]
"""
## 4. Aplicaci√≥n de Imputaci√≥n KNN
Procesamos todas las columnas con valores faltantes usando nuestra funci√≥n de imputaci√≥n KNN.
"""

# %%
# Crear copia para el procesamiento
df_clean = df.copy()

# Aplicar imputaci√≥n KNN a todas las columnas con valores faltantes
columns_with_nulls = df.columns[df.isnull().any()].tolist()
print(f"Columnas a procesar: {columns_with_nulls}")
print("="*60)

for column in columns_with_nulls:
    df_clean[column] = impute_with_knn(df_clean, column, k=5)

# %% [markdown]
"""
## 5. Verificaci√≥n de Tipos de Datos
Aseguramos que los tipos de datos finales sean coherentes con los originales.
"""

# %%
print("="*50)
print("AJUSTE DE TIPOS DE DATOS")
print("="*50)

# Restaurar tipos enteros para columnas que deber√≠an serlo
integer_columns = ['pclass', 'survived', 'sibsp', 'parch']
for col in integer_columns:
    if col in df_clean.columns:
        df_clean[col] = df_clean[col].round().astype('int64')
        print(f"'{col}' convertido a int64")

# Verificar tipos finales
print("\nTipos de datos finales:")
print(df_clean.dtypes)

# %% [markdown]
"""
## 6. Validaci√≥n de Resultados
Comparamos el estado antes y despu√©s de la imputaci√≥n para verificar que el proceso fue exitoso.
"""

# %%
print("="*50)
print("COMPARACI√ìN ANTES/DESPU√âS")
print("="*50)

# Valores nulos despu√©s
missing_after = df_clean.isnull().sum()
missing_pct_after = (df_clean.isnull().sum() / len(df_clean) * 100).round(2)

comparison = pd.DataFrame({
    'Antes_Nulos': missing_before,
    'Antes_%': missing_pct_before,
    'Despu√©s_Nulos': missing_after,
    'Despu√©s_%': missing_pct_after,
    'Diferencia': missing_before - missing_after
})

print(comparison[comparison['Antes_Nulos'] > 0])

# Verificar que no quedan nulos
total_nulls_after = df_clean.isnull().sum().sum()
print(f"\nTotal de valores nulos despu√©s de imputaci√≥n: {total_nulls_after}")

if total_nulls_after == 0:
    print("Imputaci√≥n completada exitosamente! No quedan valores faltantes.")
else:
    print("A√∫n quedan algunos valores faltantes por revisar.")

# %% [markdown]
"""
## 7. Muestra de Datos Finales
Mostramos una muestra de los datos procesados para verificar la coherencia.
"""

# %%
print("="*50)
print("MUESTRA DE DATOS FINALES")
print("="*50)
print(df_clean.head(10))

print(f"\nDimensiones finales: {df_clean.shape}")
print("Tipos de datos:")
print(df_clean.dtypes)

# %% [markdown]
"""
## 8. Exportaci√≥n de Resultados
Guardamos el dataset limpio en formato CSV para su uso posterior en an√°lisis y modelado.
"""

# %%
# Exportar dataset limpio
output_file = 'titanic_clean.csv'
df_clean.to_csv(output_file, index=False)
print(f"Dataset limpio exportado como: {output_file}")

# Estad√≠sticas finales
print("\n" + "="*50)
print("RESUMEN FINAL")
print("="*50)
print(f"‚Ä¢ Registros procesados: {len(df_clean):,}")
print(f"‚Ä¢ Columnas procesadas: {len(df_clean.columns)}")
print(f"‚Ä¢ Valores imputados: {missing_before.sum() - missing_after.sum()}")
print(f"‚Ä¢ Archivo generado: {output_file}")

# %% [markdown]
"""
## üìã Checklist de Verificaci√≥n

### ‚úÖ Pasos Completados:
- [x] Carga del dataset Titanic desde seaborn
- [x] Exploraci√≥n inicial de nulos y tipos de datos  
- [x] Separaci√≥n de columnas num√©ricas y categ√≥ricas
- [x] Implementaci√≥n de imputaci√≥n KNN para variables num√©ricas (KNeighborsRegressor)
- [x] Implementaci√≥n de imputaci√≥n KNN para variables categ√≥ricas (KNeighborsClassifier)
- [x] Estrategia anti-circularidad (exclusi√≥n de columna objetivo de features)
- [x] Pipeline de preprocesamiento (StandardScaler + OneHotEncoder)
- [x] Manejo de columnas con >70% nulos (relleno simple)
- [x] Preservaci√≥n de tipos de datos originales
- [x] Verificaci√≥n de resultados (antes/despu√©s)
- [x] Exportaci√≥n a CSV

### üîç Puntos de Validaci√≥n:
- ‚úÖ No quedan valores faltantes en el dataset final
- ‚úÖ Los tipos de datos son coherentes con los originales
- ‚úÖ Las transformaciones se aplicaron correctamente
- ‚úÖ El archivo CSV se gener√≥ exitosamente

### üìö Limitaciones y Consideraciones:
- **Circularidad**: Evitada mediante exclusi√≥n de columna objetivo de las caracter√≠sticas
- **Escalabilidad**: KNN puede ser lento en datasets muy grandes
- **Calidad**: Las imputaciones dependen de la similaridad entre observaciones
- **Par√°metros**: k=5 seleccionado emp√≠ricamente, podr√≠a optimizarse
- **Columnas cr√≠ticas**: Deck tiene muchos nulos y se maneja con estrategia especial
"""