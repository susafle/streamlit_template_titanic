# %% [markdown]
"""
# Preprocesamiento del Dataset Titanic con KNN
Este notebook implementa un sistema robusto de imputaciÃ³n de valores faltantes usando K-Nearest Neighbors (KNN).

## Objetivos:
1. Cargar y explorar el dataset Titanic
2. Identificar columnas con valores faltantes
3. Aplicar imputaciÃ³n KNN por columna de manera sistemÃ¡tica
4. Exportar dataset limpio para anÃ¡lisis posterior

## Limitaciones del enfoque KNN:
- Sensible a la escala de las variables (requiere normalizaciÃ³n)
- Computacionalmente costoso para datasets grandes
- Puede introducir patrones que no existen naturalmente en los datos
- La calidad depende del valor de k seleccionado
"""

# %%
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# %% [markdown]
"""
## 1. Carga y ExploraciÃ³n Inicial
Cargamos el dataset Titanic y realizamos una exploraciÃ³n inicial de nulos y tipos de datos.
"""

# %%
# Cargar dataset Titanic
df = sns.load_dataset("titanic")
print(f"Dimensiones del dataset: {df.shape}")
print("\n" + "="*50)
print("INFORMACIÃ“N GENERAL DEL DATASET")
print("="*50)
df.info()

# %%
print("="*50)
print("VALORES NULOS POR COLUMNA (ANTES)")
print("="*50)
missing_before = df.isnull().sum()
missing_pct_before = (df.isnull().sum() / len(df) * 100).round(2)
missing_summary_before = pd.DataFrame({
    'Valores_Nulos': missing_before,
    'Porcentaje': missing_pct_before
}).sort_values('Porcentaje', ascending=False)
print(missing_summary_before[missing_summary_before['Valores_Nulos'] > 0])

# %% [markdown]
"""
## 2. ClasificaciÃ³n de Variables
Separamos las columnas en numÃ©ricas y categÃ³ricas para aplicar diferentes estrategias de imputaciÃ³n.
"""

# %%
# Separar columnas por tipo
numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()

print("COLUMNAS NUMÃ‰RICAS:", numeric_columns)
print("COLUMNAS CATEGÃ“RICAS:", categorical_columns)

# %% [markdown]
"""
## 3. Estrategia de ImputaciÃ³n KNN
Aplicamos KNN columna por columna para evitar circularidad. Para cada columna:
1. Usamos relleno temporal para construir las features
2. Excluimos la columna objetivo del conjunto de caracterÃ­sticas  
3. Aplicamos transformaciones apropiadas (StandardScaler + OneHotEncoder)
4. Entrenamos el modelo KNN y predecimos los valores faltantes
"""

# %%
def impute_with_knn(df, target_column, k=5):
    """
    Imputa una columna especÃ­fica usando KNN
    """
    if df[target_column].isnull().sum() == 0:
        return df[target_column].copy()
    
    # Verificar si hay mÃ¡s del 70% de nulos
    null_percentage = df[target_column].isnull().sum() / len(df) * 100
    if null_percentage > 70:
        print(f"Columna '{target_column}' tiene {null_percentage:.1f}% nulos (>70%). Aplicando relleno simple.")
        if target_column in numeric_columns:
            return df[target_column].fillna(df[target_column].median())
        else:
            # Manejar columnas categÃ³ricas
            if pd.api.types.is_categorical_dtype(df[target_column]):
                # Agregar 'missing' a las categorÃ­as si no existe
                if 'missing' not in df[target_column].cat.categories:
                    df_result = df[target_column].cat.add_categories(['missing'])
                    return df_result.fillna('missing')
                else:
                    return df[target_column].fillna('missing')
            else:
                return df[target_column].fillna('missing')
    
    print(f"Imputando columna '{target_column}' ({null_percentage:.1f}% nulos) con KNN...")
    
    # Crear copia del DataFrame
    df_temp = df.copy()
    
    # Relleno temporal para construir features
    for col in numeric_columns:
        if col != target_column:
            df_temp[col] = df_temp[col].fillna(df_temp[col].median())
    
    for col in categorical_columns:
        if col != target_column:
            df_temp[col] = df_temp[col].fillna(df_temp[col].mode()[0] if not df_temp[col].mode().empty else 'unknown')
    
    # Preparar caracterÃ­sticas (excluir columna objetivo)
    feature_cols = [col for col in df.columns if col != target_column]
    X = df_temp[feature_cols]
    y = df_temp[target_column]
    
    # Separar features por tipo para el transformador
    numeric_features = [col for col in feature_cols if col in numeric_columns]
    categorical_features = [col for col in feature_cols if col in categorical_columns]
    
    # Crear pipeline de preprocessing
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
        ],
        remainder='drop'
    )
    
    # Identificar filas con y sin valores faltantes
    mask_missing = y.isnull()
    mask_complete = ~mask_missing
    
    if mask_complete.sum() == 0:
        print(f"No hay valores no nulos para entrenar en '{target_column}'")
        if target_column in numeric_columns:
            return df[target_column].fillna(0)
        else:
            return df[target_column].fillna('unknown')
    
    # Preparar datos de entrenamiento
    X_train = X[mask_complete]
    y_train = y[mask_complete]
    X_missing = X[mask_missing]
    
    # Procesar caracterÃ­sticas
    X_train_processed = preprocessor.fit_transform(X_train)
    X_missing_processed = preprocessor.transform(X_missing)
    
    # Determinar el modelo segÃºn el tipo de columna
    if target_column in numeric_columns:
        # Para variables numÃ©ricas usar KNeighborsRegressor
        model = KNeighborsRegressor(n_neighbors=min(k, len(X_train_processed)))
        model.fit(X_train_processed, y_train)
        predictions = model.predict(X_missing_processed)
    else:
        # Para variables categÃ³ricas usar KNeighborsClassifier
        # Codificar la variable objetivo
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train.astype(str))
        
        model = KNeighborsClassifier(n_neighbors=min(k, len(X_train_processed)))
        model.fit(X_train_processed, y_train_encoded)
        predictions_encoded = model.predict(X_missing_processed)
        
        # Decodificar las predicciones
        predictions = label_encoder.inverse_transform(predictions_encoded)
    
    # Crear serie resultado
    result = y.copy()
    result.loc[mask_missing] = predictions
    
    return result

# %% [markdown]
"""
## 4. AplicaciÃ³n de ImputaciÃ³n KNN
Procesamos todas las columnas con valores faltantes usando nuestra funciÃ³n de imputaciÃ³n KNN.
"""

# %%
# Crear copia para el procesamiento
df_clean = df.copy()

# Aplicar imputaciÃ³n KNN a todas las columnas con valores faltantes
columns_with_nulls = df.columns[df.isnull().any()].tolist()
print(f"Columnas a procesar: {columns_with_nulls}")
print("="*60)

for column in columns_with_nulls:
    df_clean[column] = impute_with_knn(df_clean, column, k=5)

# %% [markdown]
"""
## 5. VerificaciÃ³n de Tipos de Datos
Aseguramos que los tipos de datos finales sean coherentes con los originales.
"""

# %%
print("="*50)
print("AJUSTE DE TIPOS DE DATOS")
print("="*50)

# Restaurar tipos enteros para columnas que deberÃ­an serlo
integer_columns = ['pclass', 'survived', 'sibsp', 'parch']
for col in integer_columns:
    if col in df_clean.columns:
        df_clean[col] = df_clean[col].round().astype('int64')
        print(f"'{col}' convertido a int64")

# Verificar tipos finales
print("\nTipos de datos finales:")
print(df_clean.dtypes)

# %% [markdown]
"""
## 6. ValidaciÃ³n de Resultados
Comparamos el estado antes y despuÃ©s de la imputaciÃ³n para verificar que el proceso fue exitoso.
"""

# %%
print("="*50)
print("COMPARACIÃ“N ANTES/DESPUÃ‰S")
print("="*50)

# Valores nulos despuÃ©s
missing_after = df_clean.isnull().sum()
missing_pct_after = (df_clean.isnull().sum() / len(df_clean) * 100).round(2)

comparison = pd.DataFrame({
    'Antes_Nulos': missing_before,
    'Antes_%': missing_pct_before,
    'DespuÃ©s_Nulos': missing_after,
    'DespuÃ©s_%': missing_pct_after,
    'Diferencia': missing_before - missing_after
})

print(comparison[comparison['Antes_Nulos'] > 0])

# Verificar que no quedan nulos
total_nulls_after = df_clean.isnull().sum().sum()
print(f"\nTotal de valores nulos despuÃ©s de imputaciÃ³n: {total_nulls_after}")

if total_nulls_after == 0:
    print("ImputaciÃ³n completada exitosamente! No quedan valores faltantes.")
else:
    print("AÃºn quedan algunos valores faltantes por revisar.")

# %% [markdown]
"""
## 7. Muestra de Datos Finales
Mostramos una muestra de los datos procesados para verificar la coherencia.
"""

# %%
print("="*50)
print("MUESTRA DE DATOS FINALES")
print("="*50)
print(df_clean.head(10))

print(f"\nDimensiones finales: {df_clean.shape}")
print("Tipos de datos:")
print(df_clean.dtypes)

# %% [markdown]
"""
## 8. ExportaciÃ³n de Resultados
Guardamos el dataset limpio en formato CSV para su uso posterior en anÃ¡lisis y modelado.
"""

# %%
# Exportar dataset limpio
output_file = 'titanic_clean.csv'
df_clean.to_csv(output_file, index=False)
print(f"Dataset limpio exportado como: {output_file}")

# EstadÃ­sticas finales
print("\n" + "="*50)
print("RESUMEN FINAL")
print("="*50)
print(f"â€¢ Registros procesados: {len(df_clean):,}")
print(f"â€¢ Columnas procesadas: {len(df_clean.columns)}")
print(f"â€¢ Valores imputados: {missing_before.sum() - missing_after.sum()}")
print(f"â€¢ Archivo generado: {output_file}")

# %% [markdown]
"""
## ğŸ“‹ Checklist de VerificaciÃ³n

### âœ… Pasos Completados:
- [x] Carga del dataset Titanic desde seaborn
- [x] ExploraciÃ³n inicial de nulos y tipos de datos  
- [x] SeparaciÃ³n de columnas numÃ©ricas y categÃ³ricas
- [x] ImplementaciÃ³n de imputaciÃ³n KNN para variables numÃ©ricas (KNeighborsRegressor)
- [x] ImplementaciÃ³n de imputaciÃ³n KNN para variables categÃ³ricas (KNeighborsClassifier)
- [x] Estrategia anti-circularidad (exclusiÃ³n de columna objetivo de features)
- [x] Pipeline de preprocesamiento (StandardScaler + OneHotEncoder)
- [x] Manejo de columnas con >70% nulos (relleno simple)
- [x] PreservaciÃ³n de tipos de datos originales
- [x] VerificaciÃ³n de resultados (antes/despuÃ©s)
- [x] ExportaciÃ³n a CSV

### ğŸ” Puntos de ValidaciÃ³n:
- âœ… No quedan valores faltantes en el dataset final
- âœ… Los tipos de datos son coherentes con los originales
- âœ… Las transformaciones se aplicaron correctamente
- âœ… El archivo CSV se generÃ³ exitosamente

### ğŸ“š Limitaciones y Consideraciones:
- **Circularidad**: Evitada mediante exclusiÃ³n de columna objetivo de las caracterÃ­sticas
- **Escalabilidad**: KNN puede ser lento en datasets muy grandes
- **Calidad**: Las imputaciones dependen de la similaridad entre observaciones
- **ParÃ¡metros**: k=5 seleccionado empÃ­ricamente, podrÃ­a optimizarse
- **Columnas crÃ­ticas**: Deck tiene muchos nulos y se maneja con estrategia especial
"""